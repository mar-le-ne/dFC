{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "import numpy as np\n",
    "from nilearn import plotting\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.signal import butter, filtfilt, iirfilter, freqs\n",
    "from scipy.signal import freqz\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr\n",
    "from scipy import signal\n",
    "import os\n",
    "import subprocess\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\"./inputs/labels_reduc.txt\", sep=\",\", header=None)\n",
    "\n",
    "def savefiles(mtx, title, filename, typ):\n",
    "    \n",
    "    if (typ == \"png\") or (typ == \"pngcsv\"):\n",
    "        # save img\n",
    "        plt.matshow(mtx)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        plt.clim(0,1)\n",
    "        plt.xticks(labels[0], labels[1], rotation='vertical', fontsize=7)\n",
    "        plt.savefig(filename+\".png\")\n",
    "        plt.close()\n",
    "\n",
    "    if (typ == \"csv\") or (typ == \"pngcsv\") or (typ == \"pngcsv_c\"):\n",
    "        # save csv\n",
    "        np.savetxt(filename+\".csv\", mtx , delimiter=\",\")\n",
    "\n",
    "    if (typ == \"pngcsv_c\"):\n",
    "        # save img\n",
    "        plt.matshow(mtx)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        plt.clim(-1,1)\n",
    "        plt.xticks(labels[0], labels[1], rotation='vertical', fontsize=7)\n",
    "        plt.savefig(filename+\".png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS TO CHANGE ######\n",
    "\n",
    "group = \"CN\" # AD or CN\n",
    "\n",
    "#############################\n",
    "\n",
    "# Get the list of all files and directories\n",
    "in_path = \"./inputs/\"+group+\"_proc_roi100/\"\n",
    "dir_list = os.listdir(in_path)\n",
    "\n",
    "FC_sum = np.zeros([100, 100])\n",
    "count = 0\n",
    "S_group_windows = dict()\n",
    "CS_group = []\n",
    "for patient in dir_list:\n",
    "\n",
    "    data_set = pd.read_csv(in_path+patient, header=None)\n",
    "\n",
    "    # get time series in array\n",
    "    time_series = np.array(data_set) # (140, 400)\n",
    "    time_series = time_series[:,:100] # (140, 100)\n",
    "\n",
    "    # parameters MRI\n",
    "    N_voxels = len(time_series[1,:]) #100\n",
    "    N_TR = len(time_series[:,1])# 140 # number of repetition times\n",
    "    TR = 3 # repetition time in seconds\n",
    "\n",
    "    # Sample rate and desired cutoff frequencies (in Hz).\n",
    "    fs = 1/TR\n",
    "    lowcut = 0.01\n",
    "    highcut = 0.08\n",
    "\n",
    "    # filter\n",
    "    order = 12\n",
    "    nyq = 0.5*fs\n",
    "    b, a = butter(order, [lowcut/nyq, highcut/nyq], btype='band')\n",
    "    w, h = freqz(b, a, fs=fs)\n",
    "\n",
    "\n",
    "    # remove 1st 3 values value\n",
    "    remov = 3\n",
    "    time_series_r = time_series[remov:,:]\n",
    "    N_TR = N_TR - remov\n",
    "\n",
    "    # time\n",
    "    t = np.linspace(0,(TR*N_TR)/60, N_TR-1) # min\n",
    "\n",
    "    time_series_filt = np.zeros([N_TR, N_voxels])\n",
    "\n",
    "    # normalization\n",
    "    for parcel in range(N_voxels):\n",
    "\n",
    "        time_series_p = (time_series_r[:, parcel]  - np.mean(time_series_r[:, parcel] ))/ np.std(time_series_r[:, parcel])\n",
    "        time_series_f = filtfilt(b, a, time_series_p)\n",
    "        time_series_filt[:,parcel] = time_series_f\n",
    "\n",
    "    tseries_corr = pd.DataFrame(time_series_filt).corr(method ='pearson')\n",
    "    FC_sum = FC_sum + (tseries_corr + 1)/ 2 # 100, 100\n",
    "\n",
    "    \"\"\"Dynamic connectivity analysis\"\"\"\n",
    "\n",
    "    # sliding time–window\n",
    "    # with width of L = 20 TR slid in steps of 1 TR) approach\n",
    "\n",
    "    L = 13 # 39s = 13*3 window width\n",
    "    F = N_TR # 137 time points\n",
    "\n",
    "    # number of time windows\n",
    "    W = F - L + 1 # 125 time windows\n",
    "    \n",
    "    # calculate dFC over all regions for a finite window time\n",
    "    start_w = 0\n",
    "    t_window = []\n",
    "\n",
    "    S = dict()\n",
    "    CS = np.zeros([N_voxels, W]) # nodes, graphs\n",
    "\n",
    "    for window_step in range(W):\n",
    "\n",
    "        \"\"\" S \"\"\" # 100, 100\n",
    "        tseries_wind = pd.DataFrame(time_series_filt[start_w : start_w + L,:])\n",
    "        R_w = tseries_wind.corr(method ='pearson')\n",
    "        S_w = (R_w + 1)/ 2 # 100, 100\n",
    "        S[window_step] = S_w\n",
    "\n",
    "        \"\"\" connectivity strength of each voxel\"\"\"\n",
    "\n",
    "        CS[:,window_step] = S_w.sum(axis=1) # (100, 125)\n",
    "\n",
    "        start_w = start_w + 1\n",
    "\n",
    "    # add all windows from a specific patient to a dictionary\n",
    "    S_group_windows[count] =  S\n",
    "    CS_group.append(np.mean(CS))\n",
    "\n",
    "    #savefiles(S_w, \"Last Window Simililarity Graph\", './outputs/S_last_window/'+group+'/S_'+str(count), \"png\")\n",
    "    #savefiles(CS, \"Connectivity Strength\", './outputs/CS/'+group+'/CS_'+str(count), \"png\")\n",
    "\n",
    "    # To estimate how the brain connectivity patterns of different time–\n",
    "    # windows are associated to each other, a new similarity matrix, Scs(125 × 125)\n",
    "\n",
    "    # correlation of connectivity strength\n",
    "    Ccs_df = pd.DataFrame(CS)\n",
    "    Ccs = Ccs_df.corr(method ='pearson') #(125, 125)\n",
    "    #Scs = (cs + 1)/ 2\n",
    "    \n",
    "    savefiles(Ccs, \"Correlation Connectivity Strength\", './outputs/Ccs_100_corr/'+group+'/Ccs_'+str(count), \"pngcsv_c\")\n",
    "    \n",
    "    count = count +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary of group stationary FC\n",
    "FC = FC_sum/count\n",
    "savefiles(FC, \"Functional Connectivity \"+group+\" Group Average\", './outputs/FC_'+group, \"pngcsv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to matlab\n",
    "\n",
    "\"\"\" Average S according to Modularity \"\"\"\n",
    "\n",
    "# Since modules of Scs(125 × 125) may correspond to sets of time windows with similar\n",
    "#  brain connec\u0002tivity patterns, the modular organization of this similarity matrix,\n",
    "# Scs, is analyzed with the modularity algorithm\n",
    "\n",
    "script_path = \"modul_\"+group+\".m\"\n",
    "# Start the MATLAB subprocess\n",
    "matlab_process = subprocess.Popen(['matlab', '-nodisplay', '-nosplash', '-nodesktop', '-r', f\"run('{script_path}')\"])\n",
    "\n",
    "# sleep 1 min to make time for the matlab script to be run\n",
    "time.sleep(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get states, which are averages of similarity windows\n",
    "\n",
    "out_path_mod = \"./outputs/modularity/\"\n",
    "Ci_all = np.array(pd.read_csv(out_path_mod+\"Ci_all_\"+group+\".csv\", header=None))\n",
    "Q_all = np.array(pd.read_csv(out_path_mod+\"Q_all_\"+group+\".csv\", header=None))\n",
    "\n",
    "count = 0\n",
    "state_count = 0\n",
    "for patient in dir_list:\n",
    "\n",
    "    # max number of modules\n",
    "    max_Ci = max(Ci_all[:,count]) # count = patient\n",
    "\n",
    "    for m in (range(1,max_Ci+1)):\n",
    "        ind = [index for index, value in enumerate(Ci_all[:, count]) if value == m]\n",
    "        S_sum = np.zeros([100,100])\n",
    "        for i in ind:\n",
    "            #S_sum = S_sum + S[i]\n",
    "            S_sum = S_sum + S_group_windows[count][i]\n",
    "        ST_m = S_sum/len(ind)\n",
    "\n",
    "        # save\n",
    "        savefiles(ST_m, \"State\", './outputs/connect_state/'+group+'/ST_'+str(state_count), \"csv\")\n",
    "        savefiles(ST_m, \"State\", './outputs/connect_state/figures_'+group+'/ST_'+str(state_count), \"png\")\n",
    "        \n",
    "        state_count = state_count +1\n",
    "        \n",
    "    count = count+1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
